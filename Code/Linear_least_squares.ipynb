{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= Libraries =======\n",
    "\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"5\" # export OMP_NUM_THREADS=5\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"5\" # export OPENBLAS_NUM_THREADS=5\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"5\" # export MKL_NUM_THREADS=5\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"5\" # export VECLIB_MAXIMUM_THREADS=5\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"5\" # export NUMEXPR_NUM_THREADS=5\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "from scipy.integrate import odeint, solve_ivp\n",
    "# from torchdiffeq import odeint\n",
    "from scipy.linalg import expm, qr\n",
    "import scipy.io as sio\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device('cuda')\n",
    "# else:\n",
    "device = torch.device('cpu')\n",
    "import copy\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "import random\n",
    "# Reproducibility\n",
    "random.seed(999)\n",
    "np.random.seed(999)\n",
    "torch.manual_seed(999)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= Problem generation =======\n",
    "\n",
    "TARGET_RES      = 3e-5\n",
    "N_EXPERIMENTS   = 11\n",
    "LEARNING_RATES  = np.array(np.logspace(-0.2, 0.8, 15))\n",
    "# LEARNING_RATES  = np.append(LEARNING_RATES, np.array(np.logspace(-1.2, 8, 6)))\n",
    "iter_limit      = 30000\n",
    "\n",
    "GD_N_iter     = np.zeros((N_EXPERIMENTS, len(LEARNING_RATES)))\n",
    "SGD_N_iter    = np.zeros((N_EXPERIMENTS, len(LEARNING_RATES)))\n",
    "SPL_N_iter    = np.zeros((N_EXPERIMENTS, len(LEARNING_RATES)))\n",
    "\n",
    "GD_time     = np.zeros((N_EXPERIMENTS, len(LEARNING_RATES)))\n",
    "SGD_time    = np.zeros((N_EXPERIMENTS, len(LEARNING_RATES)))\n",
    "SPL_time    = np.zeros((N_EXPERIMENTS, len(LEARNING_RATES)))\n",
    "\n",
    "p = 500\n",
    "n = 10000\n",
    "s = 500\n",
    "b = 20\n",
    "epsilon = 1e-1\n",
    "\n",
    "# problems = ['tomography', 'random']\n",
    "problems = ['tomography']\n",
    "# problems = ['random']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= Function definitions =======\n",
    "\n",
    "def load_tom_data(epsilon=1e-5):\n",
    "    X = sio.loadmat(\"./lls_data/fanlinear.mat\")[\"A\"].toarray()\n",
    "    theta_true = sio.loadmat(\"./lls_data/shepplogan.mat\")[\"x\"]\n",
    "    n, p = X.shape\n",
    "    y = np.squeeze(X @ theta_true)\n",
    "    return X, theta_true, y\n",
    "\n",
    "def generate_problem(p, n, lstsq=False, epsilon = 0):\n",
    "    X = np.random.randn(n, p)\n",
    "\n",
    "    # Model definition\n",
    "    theta_clean = np.ones(p)\n",
    "    y = X @ theta_clean + epsilon*np.random.randn(n) # right-hand side\n",
    "    init_bound = 1.0/math.sqrt(p)\n",
    "    theta_0 = np.array(init_bound*torch.FloatTensor(p).uniform_(-1, 1))\n",
    "\n",
    "    if lstsq == True:\n",
    "        theta_lstsq = np.linalg.lstsq(X,y)[0]\n",
    "        return X, theta_0, y, theta_lstsq\n",
    "    else:\n",
    "        return X, theta_0, y\n",
    "\n",
    "def solve_local_problem(Q, R, theta_0, y_batch, h, n):\n",
    "    try:\n",
    "        R_it = np.linalg.inv(R.T)\n",
    "    except np.linalg.LinAlgError as err:\n",
    "        # print(err)\n",
    "        R_it = np.linalg.pinv(R.T)\n",
    "    exp_m = expm(-1/n* R @ R.T*h)\n",
    "    return Q @ ( exp_m @ (Q.T @ theta_0 - R_it @ y_batch )) + Q @ (R_it @ y_batch) + theta_0 - Q @ (Q.T @ theta_0)\n",
    "\n",
    "def solve_local_problem_b_1(x, theta_0, y, h, n):\n",
    "    x = x.T\n",
    "    norm = x.T @ x\n",
    "    return theta_0 + (1 - np.exp(-norm*h/n))*(y - x.T @ theta_0)/norm*x\n",
    "\n",
    "def loss(X, theta, y):\n",
    "    '''\n",
    "    Supports batch reformulation. The difference in dimension of the input\n",
    "    '''\n",
    "    if len(X.shape) == 2:\n",
    "        n, p = X.shape\n",
    "        return 1/n*np.linalg.norm(X @ theta - y)**2\n",
    "    elif len(X.shape) == 3:\n",
    "        s, b, p = Xs.shape\n",
    "        n = b*s\n",
    "\n",
    "        loss = 0\n",
    "        for i_batch in range(s):\n",
    "            loss += 1/n*np.linalg.norm(X[i_batch] @ theta - y[i_batch])**2\n",
    "        return loss\n",
    "    else:\n",
    "        raise ValueError('ðŸ¤” Inappropriate format of dataset')\n",
    "\n",
    "def rel_residual(X, theta, y):\n",
    "    '''\n",
    "    Supports batch reformulation. The difference in dimension of the input\n",
    "    '''\n",
    "    if len(X.shape) == 2:\n",
    "        n, p = X.shape\n",
    "        return np.linalg.norm(X @ theta - y)/np.linalg.norm(y)\n",
    "    elif len(X.shape) == 3:\n",
    "        s, b, p = Xs.shape\n",
    "        n = b*s\n",
    "\n",
    "        loss = 0\n",
    "        y_full = np.zeros(n)\n",
    "        X_full = np.zeros((n, p))\n",
    "        for i_batch in range(s):\n",
    "            y_full[b*i_batch:b*(i_batch+1)]     = y[i_batch]\n",
    "            X_full[b*i_batch:b*(i_batch+1), :]  = X[i_batch]\n",
    "\n",
    "        return np.linalg.norm(X_full @ theta - y_full)/np.linalg.norm(y_full)\n",
    "    else:\n",
    "        raise ValueError('ðŸ¤” Inappropriate format of dataset')\n",
    "\n",
    "\n",
    "def gradient(X, theta, y):\n",
    "    n, p = X.shape\n",
    "    return 1/n* X.T @ (X @ theta - y)\n",
    "\n",
    "def make_SGD_step(X_batch, theta_0, y_batch, lr):\n",
    "    theta = theta_0 - lr*gradient(X_batch, theta_0, y_batch)\n",
    "    return theta\n",
    "\n",
    "def sgd_training(theta_0, Xs, ys, lr, final_res = 1e-4, iter_limit = 1000):\n",
    "    s, b, p = Xs.shape\n",
    "    n = b*s\n",
    "    losses = []\n",
    "    theta_t = theta_0\n",
    "    N_iter = 0\n",
    "    stop_word = False\n",
    "    if lr >= 10:\n",
    "        iter_limit = 10000\n",
    "    while not stop_word:          \n",
    "        i_batch = N_iter % s\n",
    "        loss_t = loss(Xs, theta_t, ys)\n",
    "        losses.append(loss_t)\n",
    "        theta_t = make_SGD_step(Xs[i_batch], theta_t, ys[i_batch], lr)\n",
    "        N_iter += 1\n",
    "        if N_iter % 50 == 0:\n",
    "            sys.stdout.write('\\r'+f'ðŸ¤– SGD rel_res {rel_residual(Xs, theta_t, ys):.5f}, error {losses[-1]:.3f}/{final_res:.5f} on {N_iter}-th iteration. Lr {lr}')\n",
    "        if losses[-1] <= final_res:\n",
    "            stop_word = True\n",
    "            break\n",
    "\n",
    "        if losses[-1] >= 1e4 or N_iter >= iter_limit:\n",
    "            stop_word = True\n",
    "            N_iter = None\n",
    "    \n",
    "    print(f'\\nðŸ¤– SGD finished with {N_iter} iterations on lr {lr}')\n",
    "\n",
    "    return N_iter\n",
    "\n",
    "def spl_training(theta_0, Qs, Rs, Xs, ys, stepsize, final_res = 1e-4, iter_limit = 1000):\n",
    "    s, b, p = Xs.shape\n",
    "    n = b*s\n",
    "    losses = []\n",
    "    theta_t = theta_0\n",
    "    N_iter = 0\n",
    "    stop_word = False\n",
    "    while not stop_word:          \n",
    "        i_batch = N_iter % s\n",
    "        loss_t = loss(Xs, theta_t, ys)\n",
    "        losses.append(loss_t)\n",
    "        theta_t = solve_local_problem(Qs[i_batch], Rs[i_batch], theta_t, ys[i_batch], stepsize, n)\n",
    "        N_iter += 1\n",
    "        if N_iter % 50 == 0:\n",
    "            sys.stdout.write('\\r'+f'ðŸ¤– Splitting rel_res {rel_residual(Xs, theta_t, ys)}, error {losses[-1]:.5f}/{final_res:.5f} on {N_iter}-th iteration. Stepsize {stepsize}')\n",
    "        if losses[-1] <= final_res:\n",
    "            stop_word = True\n",
    "            break\n",
    "\n",
    "        if losses[-1] >= 1e4 or N_iter >= iter_limit:\n",
    "            stop_word = True\n",
    "            N_iter = None\n",
    "    \n",
    "    print(f'\\nðŸ¤– Splitting finished with {N_iter} iterations on Stepsize {stepsize}')\n",
    "\n",
    "    return N_iter\n",
    "\n",
    "def plot_convergence_from_lr_time(learning_rates, list_of_methods, list_of_labels):\n",
    "    colors = ['g', 'r']\n",
    "    color_labels = ['^', 'o']\n",
    "    plt.figure(figsize = (3.5,2.5))\n",
    "    for method, label, color, col_lab in zip(list_of_methods, list_of_labels, colors, color_labels):\n",
    "        mean    = np.zeros(len(learning_rates))\n",
    "        std     = np.zeros(len(learning_rates))\n",
    "\n",
    "        for i_lr, lr in enumerate(learning_rates):\n",
    "            if any(method[:, i_lr]) == None:\n",
    "                mean[i_lr] = None\n",
    "                std[i_lr]  = None\n",
    "            else:\n",
    "                mean[i_lr] = np.mean(method[:, i_lr])\n",
    "                std[i_lr]  = np.std(method[:, i_lr])\n",
    "        plt.loglog(learning_rates, mean, color+col_lab, label = label)\n",
    "        plt.loglog(learning_rates, mean, color+':')\n",
    "        plt.fill_between(learning_rates, mean-std, mean+std, color=color, alpha=0.1)\n",
    "        plt.grid(True,which=\"both\", linestyle='--', linewidth=0.4)\n",
    "        # plt.grid()\n",
    "        plt.xlabel('Learning rate')\n",
    "        plt.ylabel('Time to converge')\n",
    "        plt.legend()\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_convergence_from_lr(learning_rates, list_of_methods, list_of_labels):\n",
    "    colors = ['g', 'r']\n",
    "    color_labels = ['^', 'o']\n",
    "    plt.figure(figsize = (3.5,2.5))\n",
    "    for method, label, color, col_lab in zip(list_of_methods, list_of_labels, colors, color_labels):\n",
    "        mean    = np.zeros(len(learning_rates))\n",
    "        std     = np.zeros(len(learning_rates))\n",
    "\n",
    "        for i_lr, lr in enumerate(learning_rates):\n",
    "            if any(method[:, i_lr]) == None:\n",
    "                mean[i_lr] = None\n",
    "                std[i_lr]  = None\n",
    "            else:\n",
    "                mean[i_lr] = np.mean(method[:, i_lr])\n",
    "                std[i_lr]  = np.std(method[:, i_lr])\n",
    "        std     = np.std(method, axis = 0)   \n",
    "        plt.loglog(learning_rates, mean, color+col_lab, label = label)\n",
    "        plt.loglog(learning_rates, mean, color+':')\n",
    "        plt.fill_between(learning_rates, mean-std, mean+std, color=color, alpha=0.1)\n",
    "        plt.grid(True,which=\"both\", linestyle='--', linewidth=0.4)\n",
    "        # plt.grid()\n",
    "        plt.xlabel('Learning rate')\n",
    "        plt.ylabel('Iterations to converge')\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for problem in problems:\n",
    "    print(f'============ â˜„ {problem} â˜„ ============')\n",
    "    if problem == 'tomography':\n",
    "        X, theta_true, y = load_tom_data()\n",
    "        n, p    = X.shape\n",
    "        b, s    = 60, 213\n",
    "\n",
    "    for i_exp in progress_bar(range(N_EXPERIMENTS)):\n",
    "        print(f'============ â˜„ {i_exp+1}/ {N_EXPERIMENTS} â˜„ ============')\n",
    "        # Random initialization\n",
    "        if problem == 'tomography':\n",
    "            init_bound  = 1.0/math.sqrt(p)\n",
    "            theta_0     = np.array(init_bound*torch.FloatTensor(p).uniform_(-1, 1))\n",
    "            permutation = np.random.permutation(n)\n",
    "            X, y        = X[permutation], y[permutation]\n",
    "        elif problem == 'random':\n",
    "            X, theta_0, y, theta_lstsq = generate_problem(p,n, lstsq=True, epsilon=epsilon)\n",
    "\n",
    "        Xs = np.zeros((s, b, p))\n",
    "        ys = np.zeros((s, b))\n",
    "        Qs = np.zeros((s, p, b))\n",
    "        Rs = np.zeros((s, b, b))\n",
    "        Q, R = qr(X.T, mode='economic')\n",
    "\n",
    "        for i_batch in range(s):\n",
    "            Xs[i_batch] = X[b*i_batch:b*(i_batch+1), :]\n",
    "            ys[i_batch] = y[b*i_batch:b*(i_batch+1)]\n",
    "            Qs[i_batch], Rs[i_batch] = qr(Xs[i_batch].T, mode='economic')\n",
    "            \n",
    "        # RUN\n",
    "        for i_lr, learning_rate in enumerate(LEARNING_RATES):\n",
    "            stepsize = learning_rate*n/b\n",
    "            print(f'======ðŸŒ  lr {learning_rate}, h {stepsize} ðŸŒ ======')\n",
    "\n",
    "            start_time = time.time()\n",
    "            N_iter = spl_training(theta_0, Qs, Rs, Xs, ys, stepsize, final_res = TARGET_RES, iter_limit = iter_limit)\n",
    "            end_time = time.time()\n",
    "            SPL_time[i_exp, i_lr] = end_time - start_time\n",
    "            SPL_N_iter[i_exp, i_lr] = N_iter\n",
    "            if N_iter == None:\n",
    "                SPL_N_iter[i_exp, i_lr] = None\n",
    "\n",
    "            start_time = time.time()\n",
    "            N_iter = sgd_training(theta_0, Xs, ys, learning_rate, final_res = TARGET_RES, iter_limit = iter_limit)\n",
    "            end_time = time.time()\n",
    "            SGD_time[i_exp, i_lr] = end_time - start_time\n",
    "            SGD_N_iter[i_exp, i_lr] = N_iter\n",
    "            if N_iter == None:\n",
    "                SGD_time[i_exp, i_lr] = None\n",
    "\n",
    "            np.savez(f'LLS_{problem}_time_err{TARGET_RES}_raw.npz', SPL_time=SPL_time, SGD_time=SGD_time, LEARNING_RATES = LEARNING_RATES)\n",
    "            np.savez(f'LLS_{problem}_iter_err{TARGET_RES}_raw.npz', SPL_N_iter=SPL_N_iter, SGD_N_iter=SGD_N_iter, LEARNING_RATES = LEARNING_RATES)\n",
    "\n",
    "            plot_convergence_from_lr_time(LEARNING_RATES, [SPL_time, SGD_time], ['Splitting','SGD'])\n",
    "            plt.savefig(f'LLS_{problem}_time_err{TARGET_RES}.pdf')\n",
    "            plot_convergence_from_lr(LEARNING_RATES, [SPL_N_iter, SGD_N_iter], ['Splitting','SGD'])\n",
    "            plt.savefig(f'LLS_{problem}_iter_err{TARGET_RES}.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
