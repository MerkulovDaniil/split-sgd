{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= Libraries =======\n",
    "\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"5\" # export OMP_NUM_THREADS=5\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"5\" # export OPENBLAS_NUM_THREADS=5\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"5\" # export MKL_NUM_THREADS=5\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"5\" # export VECLIB_MAXIMUM_THREADS=5\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"5\" # export NUMEXPR_NUM_THREADS=5\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "from scipy.integrate import odeint, solve_ivp\n",
    "# from torchdiffeq import odeint\n",
    "from scipy.linalg import expm, qr\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device('cuda')\n",
    "# else:\n",
    "device = torch.device('cpu')\n",
    "import copy\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "import random\n",
    "# Reproducibility\n",
    "random.seed(999)\n",
    "np.random.seed(999)\n",
    "torch.manual_seed(999)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= Problem generation =======\n",
    "\n",
    "TARGET_ERROR    = 0.25\n",
    "N_EXPERIMENTS   = 10\n",
    "LEARNING_RATES  = np.array(np.logspace(-2, 1.5, 10))[::-1]\n",
    "# LEARNING_RATES  = np.append(LEARNING_RATES, np.array(np.logspace(1.5, 2.5, 6)))\n",
    "iter_limit      = 300000\n",
    "\n",
    "GD_time     = np.zeros((N_EXPERIMENTS, len(LEARNING_RATES)))\n",
    "SGD_time    = np.zeros((N_EXPERIMENTS, len(LEARNING_RATES)))\n",
    "SPL_time    = np.zeros((N_EXPERIMENTS, len(LEARNING_RATES)))\n",
    "\n",
    "GD_N_iter     = np.zeros((N_EXPERIMENTS, len(LEARNING_RATES)))\n",
    "SGD_N_iter    = np.zeros((N_EXPERIMENTS, len(LEARNING_RATES)))\n",
    "SPL_N_iter    = np.zeros((N_EXPERIMENTS, len(LEARNING_RATES)))\n",
    "\n",
    "# Problem generation\n",
    "batch_size = 64\n",
    "number_of_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= Function definitions =======\n",
    "\n",
    "def load_batched_data(batch_size=50, shuffle = True, qr_mode = False, number_of_classes = 3):\n",
    "    '''\n",
    "    Load batches of MNIST data.\n",
    "\n",
    "    Output: X_trains - s_train batches of training data, \n",
    "            y_trains - s_train batches of labels,\n",
    "            X_test - test points\n",
    "            y_test - test labels\n",
    "    X_trains: torch.array of shape (s_train,batch_size,*X_train[0].shape),\n",
    "        where \n",
    "        s_train - the number of batches, \n",
    "        batch_size - batch size\n",
    "        *X_train[0].shape - shape of the dataset point;\n",
    "\n",
    "    y_trains: torch.array of shape (s_train, K, batch_size),\n",
    "        where\n",
    "        K - the number of classes in the problem;\n",
    "\n",
    "    X_test: torch.array of shape (n_test,*X_train[0].shape),\n",
    "        where\n",
    "        n_test - the number of test points;\n",
    "\n",
    "    y_test: torch.array of shape (K, n_test);\n",
    "    '''\n",
    "    trainset = datasets.FashionMNIST('./fashion_mnist_data/', download=True, train=True)\n",
    "    X_train = trainset.train_data.to(dtype=torch.float)/255\n",
    "    y_train = trainset.train_labels\n",
    "    mask    = y_train < number_of_classes\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    X_train.resize_(len(X_train), *X_train[0].view(-1).shape)\n",
    "    y_train.view(-1).long()\n",
    "\n",
    "    if shuffle == True:\n",
    "        shuffling = torch.randperm(len(y_train))\n",
    "        X_train = X_train[shuffling]\n",
    "        y_train = y_train[shuffling]\n",
    "\n",
    "    # Download and load the test data\n",
    "    testset = datasets.FashionMNIST('./fashion_mnist_data/', download=True, train=False)\n",
    "    X_test = testset.test_data.to(dtype=torch.float)/255\n",
    "    y_test = testset.test_labels\n",
    "    mask   = y_test < number_of_classes\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    X_test.resize_(len(X_test), *X_test[0].view(-1).shape)\n",
    "    y_test.view(-1).long()\n",
    "\n",
    "    if shuffle == True:\n",
    "        shuffling = torch.randperm(len(y_test))\n",
    "        X_test = X_test[shuffling].to(device)\n",
    "        y_test = y_test[shuffling]\n",
    "\n",
    "    n_train = len(y_train)\n",
    "    n_test  = len(y_test)\n",
    "\n",
    "    s_train = int(n_train/batch_size)   # Number of training batches\n",
    "\n",
    "    K           = number_of_classes \n",
    "    X_trains    = torch.zeros((s_train, batch_size, *X_train[0].view(-1).shape), requires_grad=False).to(device)\n",
    "    y_trains    = torch.zeros((s_train, K, batch_size), requires_grad=False, dtype=torch.int64).to(device)\n",
    "    if qr_mode:\n",
    "        Qs      = torch.zeros((s_train, *X_train[0].view(-1).shape, batch_size), requires_grad=False).to(device)\n",
    "        Rs      = torch.zeros((s_train, batch_size, batch_size), requires_grad=False).to(device)\n",
    "        print('🤖QR started')\n",
    "\n",
    "    y_test_one_hot = torch.zeros((n_test, K))\n",
    "    y_test_one_hot[np.arange(n_test), y_test] = 1\n",
    "    y_test_one_hot = y_test_one_hot.t()\n",
    "\n",
    "    for i in range(s_train):\n",
    "        X_trains[i] = X_train[batch_size*i:batch_size*(i+1), :]\n",
    "        batch_lbls  = y_train[batch_size*i:batch_size*(i+1)]\n",
    "        y_batch_one_hot = torch.zeros((batch_size, K))\n",
    "        y_batch_one_hot[np.arange(batch_size), batch_lbls] = 1\n",
    "        y_trains[i] = y_batch_one_hot.t()\n",
    "        if qr_mode:\n",
    "            Qs[i], Rs[i] = torch.qr(X_trains[i].t())      \n",
    "\n",
    "    if qr_mode:\n",
    "        print('✅QR computed')\n",
    "        return X_trains, y_trains, X_test, y_test_one_hot, Qs, Rs\n",
    "    else:\n",
    "        return X_trains, y_trains, X_test, y_test_one_hot\n",
    "\n",
    "def load_batched_data_cifar(batch_size=50, shuffle = True, qr_mode = False, number_of_classes = 3):\n",
    "    '''\n",
    "    Load batches of MNIST data.\n",
    "\n",
    "    Output: X_trains - s_train batches of training data, \n",
    "            y_trains - s_train batches of labels,\n",
    "            X_test - test points\n",
    "            y_test - test labels\n",
    "    X_trains: torch.array of shape (s_train,batch_size,*X_train[0].shape),\n",
    "        where \n",
    "        s_train - the number of batches, \n",
    "        batch_size - batch size\n",
    "        *X_train[0].shape - shape of the dataset point;\n",
    "\n",
    "    y_trains: torch.array of shape (s_train, K, batch_size),\n",
    "        where\n",
    "        K - the number of classes in the problem;\n",
    "\n",
    "    X_test: torch.array of shape (n_test,*X_train[0].shape),\n",
    "        where\n",
    "        n_test - the number of test points;\n",
    "\n",
    "    y_test: torch.array of shape (K, n_test);\n",
    "    '''\n",
    "    trainset = datasets.CIFAR10('./cifar_data/', download=True, train=True)\n",
    "    X_train = torch.from_numpy(trainset.train_data).to(dtype=torch.float)/255\n",
    "    y_train = torch.Tensor(trainset.train_labels)\n",
    "    mask    = y_train < number_of_classes\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    X_train.resize_(len(X_train), *X_train[0].view(-1).shape)\n",
    "    y_train.view(-1).long()\n",
    "\n",
    "    if shuffle == True:\n",
    "        shuffling = torch.randperm(len(y_train))\n",
    "        X_train = X_train[shuffling]\n",
    "        y_train = y_train[shuffling]\n",
    "\n",
    "    # Download and load the test data\n",
    "    testset = datasets.CIFAR10('./cifar_data/', download=True, train=False)\n",
    "    X_test = torch.from_numpy(testset.test_data).to(dtype=torch.float)/255\n",
    "    y_test = torch.Tensor(testset.test_labels)\n",
    "    mask   = y_test < number_of_classes\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    X_test.resize_(len(X_test), *X_test[0].view(-1).shape)\n",
    "    y_test.view(-1).long()\n",
    "\n",
    "    if shuffle == True:\n",
    "        shuffling = torch.randperm(len(y_test))\n",
    "        X_test = X_test[shuffling].to(device)\n",
    "        y_test = y_test[shuffling]\n",
    "\n",
    "    n_train = len(y_train)\n",
    "    n_test  = len(y_test)\n",
    "\n",
    "    s_train = int(n_train/batch_size)   # Number of training batches\n",
    "\n",
    "    K           = number_of_classes \n",
    "    X_trains    = torch.zeros((s_train, batch_size, *X_train[0].view(-1).shape), requires_grad=False).to(device)\n",
    "    y_trains    = torch.zeros((s_train, K, batch_size), requires_grad=False, dtype=torch.int64).to(device)\n",
    "    if qr_mode:\n",
    "        Qs      = torch.zeros((s_train, *X_train[0].view(-1).shape, batch_size), requires_grad=False).to(device)\n",
    "        Rs      = torch.zeros((s_train, batch_size, batch_size), requires_grad=False).to(device)\n",
    "        print('🤖QR started')\n",
    "\n",
    "    y_test_one_hot = torch.zeros((n_test, K))\n",
    "    y_test_one_hot[np.arange(n_test), y_test.long()] = 1\n",
    "    y_test_one_hot = y_test_one_hot.t()\n",
    "\n",
    "    for i in range(s_train):\n",
    "        X_trains[i] = X_train[batch_size*i:batch_size*(i+1), :]\n",
    "        batch_lbls  = y_train[batch_size*i:batch_size*(i+1)]\n",
    "        y_batch_one_hot = torch.zeros((batch_size, K))\n",
    "        y_batch_one_hot[np.arange(batch_size), batch_lbls.long()] = 1\n",
    "        y_trains[i] = y_batch_one_hot.t()\n",
    "        if qr_mode:\n",
    "            Qs[i], Rs[i] = torch.qr(X_trains[i].t())      \n",
    "\n",
    "    if qr_mode:\n",
    "        print('✅QR computed')\n",
    "        return X_trains, y_trains, X_test, y_test_one_hot, Qs, Rs\n",
    "    else:\n",
    "        return X_trains, y_trains, X_test, y_test_one_hot\n",
    "\n",
    "def softmax_numpy(X):\n",
    "    return np.array([np.exp(x)/sum(np.exp(x)) for x in X.T]).T\n",
    "\n",
    "class CrossEntropyLoss_one_hot(nn.CrossEntropyLoss):\n",
    "    '''\n",
    "    Slightly modified version of the original CrossEntropyLoss in order to\n",
    "    handle one-hot encodings\n",
    "    '''\n",
    "    def forward(self, input, target):\n",
    "        target = torch.squeeze(torch.max(target, 1, keepdim=True)[1])\n",
    "        return F.cross_entropy(input, target, weight=self.weight,\n",
    "                                ignore_index=self.ignore_index, reduction=self.reduction)\n",
    "\n",
    "def full_problem_from_batches(Xs, ys):\n",
    "    s_train, batch_size, p = Xs.shape\n",
    "    s_train, K, batch_size = ys.shape\n",
    "    X = torch.zeros(s_train*batch_size, p)\n",
    "    y = torch.zeros(K, s_train*batch_size)\n",
    "    for i_batch in range(s_train):\n",
    "        X[batch_size*i_batch:batch_size*(i_batch+1), :] = Xs[i_batch]\n",
    "        y[:, batch_size*i_batch:batch_size*(i_batch+1)] = ys[i_batch]\n",
    "    return X, y\n",
    "\n",
    "def model_init(model, parameters_tensor):\n",
    "    new_model = copy.deepcopy(model)\n",
    "    for parameter in new_model.parameters():\n",
    "        parameter.data = parameters_tensor.clone().to(device)\n",
    "        # We won't update bias during the training, since they are not affect the model predictions\n",
    "        break\n",
    "    return new_model\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(int(input_dim), int(output_dim))\n",
    "    def forward(self, x):\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        return F.softmax(self.linear(x), dim=1)\n",
    "\n",
    "def load_model(X_test, y_test):\n",
    "    '''\n",
    "    Returns logistic regression model\n",
    "    Which is just single linear layer with flattening at the beginning and softmax at the end\n",
    "    '''\n",
    "    input_dim = X_test[0].numel()\n",
    "    K, n_test = y_test.shape\n",
    "\n",
    "    model = LogisticRegression(input_dim, K)\n",
    "    return model\n",
    "\n",
    "def gradient_flow_euler_training(theta_0, X_trains, y_trains,  X_test, y_test, lr, model, final_error = 0.2, epochs_limit = 1000):\n",
    "    X, y        = full_problem_from_batches(X_trains, y_trains)\n",
    "    X, y, X_test, y_test = X.float().to(device), y.float().to(device), X_test.to(device), y_test.to(device)\n",
    "    model = model.to(device)\n",
    "    n_train, p  = X.shape\n",
    "    K, n_test   = y_test.shape\n",
    "    thetas      = []\n",
    "    losses_train    = []\n",
    "    errors_train    = []\n",
    "    losses_test     = []\n",
    "    errors_test     = []\n",
    "    criterion       = CrossEntropyLoss_one_hot()\n",
    "    theta_t         = theta_0\n",
    "    model = model_init(model, theta_0.T)\n",
    "    stop_word = False\n",
    "    N_epochs = 0\n",
    "    while not stop_word:  \n",
    "        N_epochs += 1     \n",
    "        model.zero_grad()\n",
    "        # Forward pass\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y.T)\n",
    "        # Metrics\n",
    "        model.eval()\n",
    "        thetas.append(theta_t)\n",
    "        losses_train.append(loss.data)\n",
    "        pred_labels         = torch.max(y_pred, 1, keepdim=True)[1]\n",
    "        true_labels         = torch.max(y.T, 1, keepdim=True)[1]\n",
    "        train_acc           = true_labels.eq(pred_labels.data).sum().to(dtype=torch.float)/len(true_labels)\n",
    "        errors_train.append(1 - train_acc) \n",
    "        y_pred_test         = model(X_test)\n",
    "        loss_test           = criterion(y_pred_test, y_test.T)\n",
    "        losses_test.append(loss_test.data)\n",
    "        pred_labels_test    = torch.max(y_pred_test, 1, keepdim=True)[1]\n",
    "        true_labels_test    = torch.max(y_test.T, 1, keepdim=True)[1]\n",
    "        test_acc            = true_labels_test.eq(pred_labels_test.data).sum().to(dtype=torch.float)/len(true_labels_test)\n",
    "        errors_test.append(1 - test_acc)\n",
    "        sys.stdout.write('\\r'+f'🤖 GD error {errors_test[-1]:.3f}/{final_error:.3f} on {N_epochs}-th iteration. Lr {lr}')\n",
    "        if errors_test[-1] <= final_error or N_epochs >= epochs_limit:\n",
    "            stop_word = True\n",
    "            break\n",
    "        # Backward pass \n",
    "        model.train()\n",
    "        loss.backward()\n",
    "        for parameter in model.parameters():\n",
    "            parameter.data = parameter.data - lr*parameter.grad.data\n",
    "            theta_t = np.array((parameter.data.T).cpu())\n",
    "            break\n",
    "            \n",
    "    model.eval()\n",
    "    thetas.append(theta_t)\n",
    "    losses_train.append(loss.data)\n",
    "    pred_labels         = torch.max(y_pred, 1, keepdim=True)[1]\n",
    "    true_labels         = torch.max(y.T, 1, keepdim=True)[1]\n",
    "    train_acc           = true_labels.eq(pred_labels.data).sum().to(dtype=torch.float)/len(true_labels)\n",
    "    errors_train.append(1 - train_acc) \n",
    "    y_pred_test = model(X_test)\n",
    "    loss_test   = criterion(y_pred_test, y_test.T)\n",
    "    losses_test.append(loss_test.data)\n",
    "    pred_labels_test    = torch.max(y_pred_test, 1, keepdim=True)[1]\n",
    "    true_labels_test    = torch.max(y_test.T, 1, keepdim=True)[1]\n",
    "    test_acc            = true_labels_test.eq(pred_labels_test.data).sum().to(dtype=torch.float)/len(true_labels_test)\n",
    "    errors_test.append(1 - test_acc)\n",
    "    \n",
    "    print(f'\\n🤖 GD finished with {N_epochs} iterations on lr {lr}')\n",
    "\n",
    "    return N_epochs, thetas, losses_train,losses_test, errors_train, errors_test\n",
    "\n",
    "def sgd_training(theta_0, X_trains, y_trains,  X_test, y_test, lr, model, final_error = 0.2, iter_limit = 1000):\n",
    "    X, y        = full_problem_from_batches(X_trains, y_trains)\n",
    "    X, y, X_test, y_test = X.float().to(device), y.float().to(device), X_test.to(device), y_test.to(device)\n",
    "    model = model.to(device)\n",
    "    s_train, batch_size, p = X_trains.shape\n",
    "    n_train, p  = X.shape\n",
    "    K, n_test   = y_test.shape\n",
    "    thetas      = []\n",
    "    losses_train    = []\n",
    "    errors_train    = []\n",
    "    losses_test     = []\n",
    "    errors_test     = []\n",
    "    criterion       = CrossEntropyLoss_one_hot()\n",
    "    theta_t         = theta_0\n",
    "    model = model_init(model, theta_0.t())\n",
    "    stop_word = False\n",
    "    N_iter = 0\n",
    "    if lr >= 1:\n",
    "        iter_limit = 30000\n",
    "    if lr >= 10:\n",
    "        iter_limit = 1000\n",
    "    while not stop_word:          \n",
    "        i_batch = N_iter % s_train\n",
    "\n",
    "        if i_batch % 1 == 0:\n",
    "            # Evaluation pass\n",
    "            model.eval()\n",
    "            y_pred = model(X)\n",
    "            loss = criterion(y_pred, y.t())\n",
    "            thetas.append(theta_t)\n",
    "            losses_train.append(loss.data)\n",
    "            pred_labels     = torch.max(y_pred, 1, keepdim=True)[1]\n",
    "            true_labels     = torch.max(y.t(), 1, keepdim=True)[1]\n",
    "            train_acc       = true_labels.eq(pred_labels.data).sum().to(dtype=torch.float)/len(true_labels)\n",
    "            errors_train.append(1 - train_acc) \n",
    "            y_pred_test = model(X_test)\n",
    "            loss_test   = criterion(y_pred_test, y_test.t())\n",
    "            losses_test.append(loss_test.data)\n",
    "            pred_labels_test    = torch.max(y_pred_test, 1, keepdim=True)[1]\n",
    "            true_labels_test    = torch.max(y_test.t(), 1, keepdim=True)[1]\n",
    "            test_acc            = true_labels_test.eq(pred_labels_test.data).sum().to(dtype=torch.float)/len(true_labels_test)\n",
    "            errors_test.append(1 - test_acc)\n",
    "            sys.stdout.write('\\r'+f'🤖 SGD error {errors_test[-1]:.3f}/{final_error:.3f} on {N_iter}-th iteration. Lr {lr}')\n",
    "            if errors_test[-1] <= final_error:\n",
    "                stop_word = True\n",
    "                break\n",
    "\n",
    "            if N_iter >= iter_limit:\n",
    "                N_iter = None\n",
    "                print(f'\\n🤖 SGD Failed on lr {lr}')\n",
    "                return N_iter, thetas, losses_train,losses_test, errors_train, errors_test\n",
    "\n",
    "        # Backward pass\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        # Forward pass\n",
    "        y_pred = model(X_trains[i_batch])\n",
    "        loss = criterion(y_pred, y_trains[i_batch].t())\n",
    "        loss.backward()\n",
    "        for parameter in model.parameters():\n",
    "            parameter.data = parameter.data - lr*parameter.grad.data\n",
    "            theta_t = np.array((parameter.data.t()).cpu())\n",
    "            break\n",
    "        N_iter += 1\n",
    "\n",
    "    \n",
    "    print(f'\\n🤖 SGD finished with {N_iter} iterations on lr {lr}')\n",
    "\n",
    "    return N_iter, thetas, losses_train,losses_test, errors_train, errors_test\n",
    "\n",
    "def make_splitting_step(theta_0, Q, R, y, h, n_train):\n",
    "    p, K = theta_0.shape\n",
    "    p, batch_size = Q.shape\n",
    "    Q, R, y = np.array(Q), np.array(R), np.array(y)\n",
    "    h_seq = [0, h]\n",
    "    theta_0 = np.array(theta_0)   \n",
    "    H_0 = np.array(Q.T @ theta_0)\n",
    "    H_0_vec = H_0.ravel('F')\n",
    "    H_h = np.zeros((batch_size, K))\n",
    "\n",
    "    def rhs_vec(H, t):\n",
    "        H = H.reshape((batch_size, K), order='F')\n",
    "        rhs = -1/n_train * R@(softmax_numpy(H.T@R) - y).T\n",
    "        return rhs.ravel('F')\n",
    "        \n",
    "    H_h_vec = odeint(rhs_vec, H_0_vec, h_seq)[-1]\n",
    "    H_h = H_h_vec.reshape((batch_size, K), order='F')\n",
    "\n",
    "    theta = Q@(H_h - H_0) + theta_0\n",
    "    return torch.from_numpy(theta)\n",
    "\n",
    "def spl_training(theta_0, Qs, Rs, X_trains, y_trains,  X_test, y_test, stepsize, model, final_error = 0.2, iter_limit = 1000):\n",
    "    X, y        = full_problem_from_batches(X_trains, y_trains)\n",
    "    X, y, X_trains, y_trains, X_test, y_test, model = X.float().to(device), y.float().to(device), X_trains.float().to(device), y_trains.float().to(device), X_test.float().to(device), y_test.float().to(device), model.to(device)\n",
    "    s_train, batch_size, p = X_trains.shape\n",
    "    n_train, p  = X.shape\n",
    "    K, n_test   = y_test.shape\n",
    "    thetas      = []\n",
    "    losses_train    = []\n",
    "    errors_train    = []\n",
    "    losses_test     = []\n",
    "    errors_test     = []\n",
    "    criterion       = CrossEntropyLoss_one_hot()\n",
    "    theta_t         = theta_0.to(device)\n",
    "    model = model_init(model, theta_0.t())\n",
    "    stop_word = False\n",
    "    N_iter = 0\n",
    "    if stepsize >= 1000:\n",
    "        iter_limit = 1000\n",
    "    while not stop_word:\n",
    "        i_batch = N_iter % s_train\n",
    "\n",
    "        if i_batch % 1 == 0:      \n",
    "            # Evaluation pass\n",
    "            model.eval()\n",
    "            y_pred = model(X)\n",
    "            loss = criterion(y_pred, y.t())\n",
    "            thetas.append(theta_t)\n",
    "            losses_train.append(loss.data)\n",
    "            pred_labels         = torch.max(y_pred, 1, keepdim=True)[1]\n",
    "            true_labels         = torch.max(y.t(), 1, keepdim=True)[1]\n",
    "            train_acc           = true_labels.eq(pred_labels.data).sum().to(dtype=torch.float)/len(true_labels)\n",
    "            errors_train.append(1 - train_acc) \n",
    "            y_pred_test         = model(X_test)\n",
    "            loss_test           = criterion(y_pred_test, y_test.t())\n",
    "            losses_test.append(loss_test.data)\n",
    "            pred_labels_test    = torch.max(y_pred_test, 1, keepdim=True)[1]\n",
    "            true_labels_test    = torch.max(y_test.t(), 1, keepdim=True)[1]\n",
    "            test_acc            = true_labels_test.eq(pred_labels_test.data).sum().to(dtype=torch.float)/len(true_labels_test)\n",
    "            errors_test.append(1 - test_acc)\n",
    "            sys.stdout.write('\\r'+f'🤖 Splitting error {errors_test[-1]:.3f}/{final_error:.3f} on {N_iter}-th iteration. Stepsize {stepsize}')\n",
    "            if errors_test[-1] <= final_error:\n",
    "                stop_word = True\n",
    "                break\n",
    "\n",
    "            if N_iter >= iter_limit:\n",
    "                N_iter = None\n",
    "                print(f'\\n🤖 Splitting Failed on Stepsize {stepsize}')\n",
    "                return N_iter, thetas, losses_train,losses_test, errors_train, errors_test\n",
    "\n",
    "\n",
    "        # Backward pass\n",
    "        model.train()\n",
    "        theta_t = make_splitting_step(theta_t.cpu(), Qs[i_batch].cpu(), Rs[i_batch].cpu(), y_trains[i_batch].cpu(), stepsize, n_train).to(dtype=torch.float)\n",
    "        model = model_init(model, theta_t.t())\n",
    "        N_iter += 1  \n",
    "\n",
    "    print(f'\\n🤖 Splitting finished with {N_iter} iterations on Stepsize {stepsize}')\n",
    "\n",
    "    return N_iter, thetas, losses_train,losses_test, errors_train, errors_test\n",
    "\n",
    "def plot_convergence_from_lr_time(learning_rates, list_of_methods, list_of_labels):\n",
    "    colors = ['g', 'r']\n",
    "    color_labels = ['^', 'o']\n",
    "    plt.figure(figsize = (3.5,2.5))\n",
    "    for method, label, color, col_lab in zip(list_of_methods, list_of_labels, colors, color_labels):\n",
    "        mean    = np.zeros(len(learning_rates))\n",
    "        std     = np.zeros(len(learning_rates))\n",
    "\n",
    "        for i_lr, lr in enumerate(learning_rates):\n",
    "            if any(method[:, i_lr]) == None:\n",
    "                mean[i_lr] = None\n",
    "                std[i_lr]  = None\n",
    "            else:\n",
    "                mean[i_lr] = np.mean(method[:, i_lr])\n",
    "                std[i_lr]  = np.std(method[:, i_lr])\n",
    "        plt.loglog(learning_rates, mean, color+col_lab, label = label)\n",
    "        plt.loglog(learning_rates, mean, color+':')\n",
    "        plt.fill_between(learning_rates, mean-std, mean+std, color=color, alpha=0.1)\n",
    "        plt.grid(True,which=\"both\", linestyle='--', linewidth=0.4)\n",
    "        # plt.grid()\n",
    "        plt.xlabel('Learning rate')\n",
    "        plt.ylabel('Time to converge')\n",
    "        plt.legend()\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_convergence_from_lr(learning_rates, list_of_methods, list_of_labels):\n",
    "    colors = ['g', 'r']\n",
    "    color_labels = ['^', 'o']\n",
    "    plt.figure(figsize = (3.5,2.5))\n",
    "    for method, label, color, col_lab in zip(list_of_methods, list_of_labels, colors, color_labels):\n",
    "        mean    = np.zeros(len(learning_rates))\n",
    "        std     = np.zeros(len(learning_rates))\n",
    "\n",
    "        for i_lr, lr in enumerate(learning_rates):\n",
    "            if any(method[:, i_lr]) == None:\n",
    "                mean[i_lr] = None\n",
    "                std[i_lr]  = None\n",
    "            else:\n",
    "                mean[i_lr] = np.mean(method[:, i_lr])\n",
    "                std[i_lr]  = np.std(method[:, i_lr])\n",
    "        std     = np.std(method, axis = 0)   \n",
    "        plt.loglog(learning_rates, mean, color+col_lab, label = label)\n",
    "        plt.loglog(learning_rates, mean, color+':')\n",
    "        plt.fill_between(learning_rates, mean-std, mean+std, color=color, alpha=0.1)\n",
    "        plt.grid(True,which=\"both\", linestyle='--', linewidth=0.4)\n",
    "        # plt.grid()\n",
    "        plt.xlabel('Learning rate')\n",
    "        plt.ylabel('Iterations to converge')\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trains, y_trains, X_test, y_test, Qs, Rs = load_batched_data(batch_size=batch_size, qr_mode = True, number_of_classes=number_of_classes)\n",
    "s_train, batch_size, p = X_trains.shape # Yes, here we have bs the same as input parameter in the previous line.\n",
    "n_train, K, n_test = s_train*batch_size, *y_test.shape\n",
    "\n",
    "print('🐱 Data loaded')\n",
    "\n",
    "model = load_model(X_test, y_test)\n",
    "print('🐱 Model loaded')\n",
    "\n",
    "for i_exp in progress_bar(range(N_EXPERIMENTS)):\n",
    "    print(f'============ ☄ {i_exp+1}/ {N_EXPERIMENTS} ☄ ============')\n",
    "    # Random initialization\n",
    "    init_bound = 1.0/math.sqrt(p)\n",
    "    theta_0 = init_bound*torch.FloatTensor(p, K).uniform_(-1, 1)\n",
    "    \n",
    "    # RUN\n",
    "    for i_lr, learning_rate in enumerate(LEARNING_RATES):\n",
    "        stepsize = learning_rate*n_train/batch_size\n",
    "        print(f'======🌠 lr {learning_rate}, h {stepsize} 🌠======')\n",
    "        \n",
    "        # N_iter, thetas, losses_train,losses_test, errors_train, errors_test = \\\n",
    "        #     gradient_flow_euler_training(theta_0, X_trains, y_trains,  X_test, y_test, learning_rate, model, final_error = TARGET_ERROR)\n",
    "        # GD_time[i_exp, i_lr] = N_iter\n",
    "\n",
    "        start_time = time.time()\n",
    "        N_iter, thetas, losses_train,losses_test, errors_train, errors_test = \\\n",
    "            spl_training(theta_0,  Qs, Rs, X_trains, y_trains,  X_test, y_test, stepsize, model, final_error = TARGET_ERROR, iter_limit=iter_limit)\n",
    "        end_time = time.time()\n",
    "        SPL_time[i_exp, i_lr] = end_time - start_time\n",
    "        SPL_N_iter[i_exp, i_lr] = N_iter\n",
    "        if N_iter == None:\n",
    "            SPL_N_iter[i_exp, i_lr] = None\n",
    "\n",
    "        start_time = time.time()\n",
    "        N_iter, thetas, losses_train,losses_test, errors_train, errors_test = \\\n",
    "            sgd_training(theta_0, X_trains, y_trains,  X_test, y_test, learning_rate, model, final_error = TARGET_ERROR, iter_limit=iter_limit)\n",
    "        end_time = time.time()\n",
    "        SGD_time[i_exp, i_lr] = end_time - start_time\n",
    "        SGD_N_iter[i_exp, i_lr] = N_iter\n",
    "\n",
    "        if N_iter == None:\n",
    "            SGD_time[i_exp, i_lr] = None\n",
    "\n",
    "        np.savez(f'Softmax_fashion_mnist_iter_err{TARGET_ERROR}_raw.npz', SPL_N_iter=SPL_N_iter, SGD_N_iter=SGD_N_iter, LEARNING_RATES = LEARNING_RATES)\n",
    "        np.savez(f'Softmax_fashion_mnist_time_err{TARGET_ERROR}_raw.npz', SPL_time=SPL_time, SGD_time=SGD_time, LEARNING_RATES = LEARNING_RATES)\n",
    "\n",
    "        plot_convergence_from_lr_time(LEARNING_RATES, [SPL_time, SGD_time], ['Splitting','SGD'])\n",
    "        plt.savefig(f'Softmax_fashion_mnist_time_err{TARGET_ERROR}.pdf')\n",
    "        plot_convergence_from_lr(LEARNING_RATES, [SPL_N_iter, SGD_N_iter], ['Splitting','SGD'])\n",
    "        plt.savefig(f'Softmax_fashion_mnist_iter_err{TARGET_ERROR}.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
