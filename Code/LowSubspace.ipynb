{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LowSubspace.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"rSp07uJ6z3rH","colab_type":"text"},"source":["# Accurate local problem\n","\n","**Local goal:** create some procedure to determine learning rate scheduler for SGD.\n","\n","$$\n","\\tag{Loss}\n","f(\\theta) \\to \\min_{\\theta \\in \\mathbb{R}^p}\n","$$\n","\n","$$\n","\\tag{GF}\n","\\dfrac{d \\theta}{d t} = - \\dfrac{1}{n} \\sum_{i = 1}^s \\nabla f_i (\\theta)\n","$$\n","$n$ - the number of points, $s$ - the number of batches, $b*s = n, b$ - the batch size.\n"]},{"cell_type":"markdown","metadata":{"id":"aPVMKSv46kz9","colab_type":"text"},"source":["## Linear least squares (LLS)\n","\n","$$\n","\\tag{LLS.Loss}\n","f(\\theta) = \\dfrac{1}{n}  \\|X\\theta - y\\|^2_2\n","$$\n","\n","$$\n","\\tag{LLS.GF}\n","\\dfrac{d \\theta}{d t} = - \\dfrac{1}{n} \\sum_{i = 1}^n x_i^*(x_i^*\\theta - y_i)\n","$$\n","\n","$\\theta \\in \\mathbb{R}^p, X \\in \\mathbb{R}^{n \\times p}, X_i \\in \\mathbb{R}^{b \\times p}, y_i \\in \\mathbb{R}^b$"]},{"cell_type":"markdown","metadata":{"id":"JTpYKw397wvZ","colab_type":"text"},"source":["## Binary logistic regression (LogReg)\n","\n","$$\n","\\tag{LogReg.Loss}\n","f(\\theta) = -\\dfrac{1}{n} \\sum\\limits_{i=1}^n \\left( y_i \\ln h_\\theta(x_i) + (1-y_i) \\ln (1- h_\\theta(x_i))\\right)\n","$$\n","\n","$$\n","\\tag{LogReg.GF}\n","\\dfrac{d \\theta}{d t} = - \\dfrac{1}{n} \\sum\\limits_{i=1}^n x_i^* (h_\\theta(x_i^*) - y_i)\n","$$\n","\n","$h_\\theta(x_i) = \\dfrac{1}{1 + e^{-\\theta^*x_i}}$ - the sigmoid function, $\\theta \\in \\mathbb{R}^p, X \\in \\mathbb{R}^{n \\times p}, X_i \\in \\mathbb{R}^{b \\times p}, y_i \\in \\mathbb{R}^b$"]},{"cell_type":"markdown","metadata":{"id":"3SM2EFtl_w_l","colab_type":"text"},"source":["# Approach\n","\n","Each batch gradient (or, stochastic gradient) has rank $b$. In case of unit batchsize, we have rank one stochastic gradient.\n","\n","We will search for some scalar function $g (\\eta)$, s.t.\n","$$\n","\\tag{Approach}\n","\\dfrac{d \\theta}{d t} \\sim q_i \\cdot g(\\eta_i), \\quad\n","$$\n","\n","where $q_i$ is the basis vector of some low rank subspace of $i$-th batch gradient of batch of the number $i$."]},{"cell_type":"markdown","metadata":{"id":"No8u7hxcq1wN","colab_type":"text"},"source":["# Code\n","\n","## Softmax on MNIST"]},{"cell_type":"code","metadata":{"id":"0rmoJKD7BVeF","colab_type":"code","colab":{}},"source":["import torchvision.datasets as datasets\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","import numpy as np\n","from tqdm import tqdm\n","from matplotlib import pyplot as plt\n","import copy\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","class LogisticRegression(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super(LogisticRegression, self).__init__()\n","        self.linear = nn.Linear(int(input_dim), int(output_dim))\n","    def forward(self, x):\n","    \tx = x.contiguous().view(x.size(0), -1)\n","    \tout = F.softmax(self.linear(x), dim=1)\n","    \treturn out\n","\n","def load_model(X_test, y_test):\n","\t'''\n","\tReturns logistic regression model\n","\tWhich is just single linear layer with flattening at the beginning and softmax at the end\n","\t'''\n","\tinput_dim = X_test[0].numel()\n","\n","\t# Handling with usual and one-hot format\n","\tif any(y_test > 1):\n","\t\toutput_dim = max(y_test) + 1\n","\telse:\n","\t\toutput_dim = 2\n","\n","\tmodel = LogisticRegression(input_dim, output_dim)\n","\treturn model\n","\n","def load_batched_data(batch_size=50, shuffle = True):\n","\t'''\n","\tLoad batches of MNIST data.\n","\n","\tOutput: X_trains - N_train batches of training data, \n","\t\t\ty_trains - N_train batches of labels,\n","\t\t\tX_test - test points\n","\t\t\ty_test - test labels\n","\tX_trains: torch.array of shape (N_train,batch_size,*X_train[0].shape),\n","\t\twhere \n","\t\tN_train - the number of batches, \n","\t\tbatch_size - batch size\n","\t\t*X_train[0].shape - shape of the dataset point;\n","\n","\ty_trains: torch.array of shape (N_train,batch_size);\n","\n","\tX_test: torch.array of shape (s_test,*X_train[0].shape),\n","\t\twhere\n","\t\ts_test - the number of test points;\n","\n","\ty_test: torch.array of shape (s_test);\n","\t'''\n","\n","\ttrainset = datasets.MNIST('./mnist_data/', download=True, train=True)\n","\tX_train = trainset.data.to(dtype=torch.float)/255\n","\ty_train = trainset.targets\n","\tmask    = y_train <=1\n","\tX_train = X_train[mask]\n","\ty_train = y_train[mask]\n","\tX_train.resize_(len(X_train), 1, *X_train[0].shape)\n","\ty_train.view(-1).long()\n","\n","\tif shuffle == True:\n","\t\tshuffling = torch.randperm(len(y_train))\n","\t\tX_train = X_train[shuffling]\n","\t\ty_train = y_train[shuffling]\n","\n","\t# Download and load the test data\n","\ttestset = datasets.MNIST('./mnist_data/', download=True, train=False)\n","\tX_test = testset.data.to(dtype=torch.float)/255\n","\ty_test = testset.targets\n","\tmask   = y_test <=1\n","\tX_test = X_test[mask]\n","\ty_test = y_test[mask]\n","\tX_test.resize_(len(X_test), 1, *X_test[0].shape)\n","\ty_test.view(-1).long()\n","\n","\tif shuffle == True:\n","\t\tshuffling = torch.randperm(len(y_test))\n","\t\tX_test = X_test[shuffling].to(device)\n","\t\ty_test = y_test[shuffling]\n","\n","\ts_train = len(y_train)\n","\ts_test  = len(y_test)\n","\n","\tN_train = int(s_train/batch_size)   # Number of training batches\n","\n","\tX_trains = torch.zeros((N_train,batch_size,*X_train[0].shape),requires_grad=False).to(device)\n","\ty_trains = torch.zeros((N_train,batch_size),requires_grad=False, dtype=torch.int64).to(device)\n","\n","\tfor i in range(N_train):\n","\t    X_trains[i] = X_train[batch_size*i:batch_size*(i+1), :]\n","\t    y_trains[i] = y_train[batch_size*i:batch_size*(i+1)]\n","\n","\treturn X_trains, y_trains, X_test, y_test\n","\n","def train(model, X_trains, y_trains, X_test, y_test, n_epochs, hs, weights_name = './'):\n","\tmodel     = model.to(device)\n","\tcriterion = nn.CrossEntropyLoss()\n","\tN_batches  = int(X_trains.size(0))\n","\tbatch_size = int(X_trains.size(1)) \n","\n","\thistory = {}  \n","\thistory['train_loss'] = np.zeros(n_epochs)\n","\thistory['test_loss']  = np.zeros(n_epochs)\n","\thistory['train_acc']  = np.zeros(n_epochs)\n","\thistory['test_acc']   = np.zeros(n_epochs)\n","\n","\tfor epoch in tqdm(range(n_epochs)):\n","\t\t#==== Forward steps  ====\n","\t\tmodel.train()\n","\t\th = hs[epoch]\n","\t\tfor i_batch in range(N_batches):\n","\t\t\t# Forward pass\n","\t\t\timages, labels = X_trains[i_batch], y_trains[i_batch]\n","\t\t\timages, labels = images.to(device), labels.to(device)\n","\t\t\tpredictions = model(images)\n","\t\t\tloss = criterion(predictions, labels)\n","\n","\t\t\t# Gradient step\n","\t\t\tmodel.zero_grad()\n","\t\t\tloss.backward()\n","\t\t\t# torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n","\t\t\tfor idp, parameter in enumerate(model.parameters()):\n","\t\t\t\tgrad           = parameter.grad.data\n","\t\t\t\tgrad           = grad.to(device)\n","\t\t\t\tparameter.data = parameter.data - h*grad\n","\n","\t\t# Metrics calculation\n","\t\thistory['train_loss'][epoch] = loss.data\n","\t\tpred_label = torch.max(predictions, 1, keepdim=True)[1]\n","\t\thistory['train_acc'][epoch] = pred_label.eq(labels.data.view_as(pred_label)).sum().to(dtype=torch.float)/len(labels)\n","\t\twith torch.no_grad():\n","\t\t\tmodel.eval()\n","\t\t\tXst = X_test.to(device)\n","\t\t\tyst = y_test.to(device)\n","\t\t\tpredictions = model(Xst)\n","\t\t\tloss = criterion(predictions, yst)\n","\t\t\thistory['test_loss'][epoch] = loss.data\n","\t\t\tpred_label = torch.max(predictions, 1, keepdim=True)[1]\n","\t\t\thistory['test_acc'][epoch] = pred_label.eq(yst.data.view_as(pred_label)).sum().to(dtype=torch.float)/len(yst)\n","\treturn history\n","\n","def weight_init(m):\n","    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n","        torch.nn.init.xavier_uniform_(m.weight.data)\n","\n","def model_init(model, parameter_list):\n","\tnew_model = copy.deepcopy(model)\n","\tfor idp, parameter in enumerate(new_model.parameters()):\n","\t\tparameter.data = parameter_list[idp].to(device)\n","\treturn new_model\n","\n","def vector_of_parameters(model, parameter_column):\n","\tparams = list(model.parameters())\n","\tW, b = params\n","\treturn W[parameter_column, :]\n","\n","def calculate_batch_gradient(model, X_train, y_train, n, parameter_column):\n","\tmodel     = model.to(device)\n","\tcriterion = nn.CrossEntropyLoss()\n","\tbatch_size  = int(X_train.size(0))\n","\tmodel.train()\n","\timages, labels = X_train.to(device), y_train.to(device)\n","\tpredictions = model(images)\n","\tloss = criterion(predictions, labels)\n","\n","\t# Gradient step\n","\tmodel.zero_grad()\n","\tloss.backward()\n","\n","\tgrads = []\n","\tnormi = -batch_size/n\n","\tfor parameter in model.parameters():\n","\t\tgrads.append(normi*parameter.grad.data)\n","\n","\tvec_grad = torch.Tensor().to(device)\n","\tW_grad, b_grad = grads\n","\tvec_grad = W_grad[parameter_column, :]\n","\treturn vec_grad\n","\n","def calculate_main_singular_vector(model, X_train, y_train, n, parameter_column, N_points = 100):\n","\tbatch_gradient = []\n","\tparameter_list = list(model.parameters())\n","\tparameter_list_corrupted = [0 for i in range(len(parameter_list))] # Duplication of length\n","\tfor i_gradient in range(N_points):\n","\t\tfor idp, parameter in enumerate(parameter_list):\n","\t\t\tparameter_list_corrupted[idp] = parameter_list[idp] + 1e-2*torch.rand(*parameter_list[idp].shape).to(device)\n","\t\tmodel_corr = model_init(model, parameter_list_corrupted)\n","\t\tbatch_gradient.append(calculate_batch_gradient(model_corr, X_train, y_train, n, parameter_column))\n","\tbatch_gradient_matrix = torch.stack(batch_gradient).transpose(0,1)\n","\tu, s, v = torch.svd(batch_gradient_matrix)\n","\treturn u[:, 0]\n","\n","def plot_projection_picture(model, X_train, y_train, n, parameter_column, N_points = 100, plot = True):\n","\tq = calculate_main_singular_vector(model, X_train, y_train, n, parameter_column)\n","\t# q = X_train.view(-1)\n","\tparameter_list = list(model.parameters())\n","\tparameter_list_corrupted = [0 for i in range(len(parameter_list))] # Duplication of length\n","\tPi_q_theta = []\n","\tPi_q_grads = []\n","\n","\tfor i in range(N_points):\n","\t\tfor idp, parameter in enumerate(parameter_list):\n","\t\t\tparameter_list_corrupted[idp] = parameter_list[idp] + 1e-2*torch.rand(*parameter_list[idp].shape).to(device)\n","\t\tmodel_corr = model_init(model, parameter_list_corrupted)\n","\t\ttheta = vector_of_parameters(model_corr, parameter_column)\n","\t\tgrad = calculate_batch_gradient(model_corr, X_train, y_train, n, parameter_column)\n","\n","\t\tPi_q_theta.append(torch.dot(q, theta))\n","\t\tPi_q_grads.append(torch.dot(q, grad))\n","  \n","\tif plot == False:\n","\t\treturn np.array(Pi_q_theta), np.array(Pi_q_grads)\n","\n","\tplt.title('Softmax regression on MNIST') \n","\tplt.xlabel(r'$\\Pi_q(\\theta)$')\n","\tplt.ylabel(r'$\\Pi_q(\\nabla f_i)$')\n","\tplt.plot(Pi_q_theta, Pi_q_grads, 'bx')\n","\tplt.show()\n","\treturn None"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zr9SOpj9JHaG","colab_type":"code","outputId":"f9f52b0a-defb-43d4-c5dd-7f6d44f2f06d","executionInfo":{"status":"ok","timestamp":1567023196351,"user_tz":-180,"elapsed":1793,"user":{"displayName":"bratishka mipt","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBAUub0WXsyn4z2jHLI6zDgl-4Zx4vBBGMls8lfZA=s64","userId":"17142413044964926720"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["batch_size = 64\n","X_trains, y_trains, X_test, y_test = load_batched_data(batch_size=batch_size)\n","n = y_trains.shape[0]*y_trains.shape[1]\n","N_batches = n//batch_size\n","model = load_model(X_test, y_test)\n","model.apply(weight_init)\n","\n","n_epochs = 6\n","h = 0.005\n","train(model, X_trains, y_trains, X_test, y_test, n_epochs, [h for i in range(n_epochs)])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","\n","\n","\n","\n","\n","  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n"," 17%|█▋        | 1/6 [00:00<00:00,  6.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n"," 33%|███▎      | 2/6 [00:00<00:00,  5.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n"," 50%|█████     | 3/6 [00:00<00:00,  5.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n"," 67%|██████▋   | 4/6 [00:00<00:00,  5.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n"," 83%|████████▎ | 5/6 [00:00<00:00,  5.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","100%|██████████| 6/6 [00:01<00:00,  5.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["{'test_acc': array([0.99810874, 0.99810874, 0.99810874, 0.99810874, 0.99810874,\n","        0.99810874]),\n"," 'test_loss': array([0.36985454, 0.34492531, 0.33591232, 0.33119485, 0.32826746,\n","        0.3262617 ]),\n"," 'train_acc': array([1., 1., 1., 1., 1., 1.]),\n"," 'train_loss': array([0.37143812, 0.34732535, 0.33852497, 0.33385476, 0.33091587,\n","        0.32887518])}"]},"metadata":{"tags":[]},"execution_count":132}]},{"cell_type":"code","metadata":{"id":"1OGgxfcqK_wa","colab_type":"code","colab":{}},"source":["parameter_column = 0\n","i_batch = 0\n","Pi_q_theta, Pi_q_grads = plot_projection_picture(model, X_trains[i_batch], y_trains[i_batch], n, parameter_column, plot=False)\n","for i_batch in tqdm(range(1, N_batches)):\n","    Pi_q_theta_new, Pi_q_grads_new = plot_projection_picture(model, X_trains[i_batch], y_trains[i_batch], n, parameter_column, plot=False)\n","    Pi_q_theta = (i_batch-1)/i_batch* Pi_q_theta + 1/i_batch * Pi_q_theta_new\n","    Pi_q_grads = (i_batch-1)/i_batch* Pi_q_grads + 1/i_batch * Pi_q_grads_new"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QsXuwkdmQsvj","colab_type":"code","outputId":"4e9ab341-793b-4165-928d-4bdcd53b33ad","executionInfo":{"status":"error","timestamp":1567430628090,"user_tz":-180,"elapsed":1421,"user":{"displayName":"bratishka mipt","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBAUub0WXsyn4z2jHLI6zDgl-4Zx4vBBGMls8lfZA=s64","userId":"17142413044964926720"}},"colab":{"base_uri":"https://localhost:8080/","height":244}},"source":["plt.title('Softmax regression on MNIST') \n","plt.xlabel(r'$\\Pi_q(\\theta)$')\n","plt.ylabel(r'$\\Pi_q(\\nabla f_i)$')\n","plt.plot(Pi_q_theta, Pi_q_grads, 'bx')\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-b23bcb2f6480>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Softmax regression on MNIST'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'$\\Pi_q(\\theta)$'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'$\\Pi_q(\\nabla f_i)$'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPi_q_theta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPi_q_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"]}]},{"cell_type":"code","metadata":{"id":"MbvCQzvJpiNR","colab_type":"code","outputId":"80ebfcb1-b345-4030-d245-348df8aeda99","executionInfo":{"status":"ok","timestamp":1566835980009,"user_tz":-180,"elapsed":1456,"user":{"displayName":"bratishka mipt","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBAUub0WXsyn4z2jHLI6zDgl-4Zx4vBBGMls8lfZA=s64","userId":"17142413044964926720"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["y_trains.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([395, 32])"]},"metadata":{"tags":[]},"execution_count":138}]},{"cell_type":"code","metadata":{"id":"u5NrWrVss5vY","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}